{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project task 01: Duplicate detection with LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import re\n",
    "import json\n",
    "\n",
    "from scipy.sparse import linalg as spl\n",
    "from collections import defaultdict\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this task is to find duplicate reviews in the Yelp dataset. You can imagine a scenario were the **same** review appears for a restaurant with only small variation (e.g. few words are changed), or some user is trying to submit spam reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be working with reviews for restaurants in Las Vegas. \n",
    "\n",
    "The data includes:\n",
    "* ([download link](https://syncandshare.lrz.de/dl/fiUuDugamX2JCH33hkxYXgdF)) the preprocessed data matrix: `bow_subset.npz`, \n",
    "* ([download link](https://syncandshare.lrz.de/dl/fiMX6taWPMEQ9aaznq4oadyq)) the words in our vocabulary: `top_25k_words.npy`, \n",
    "* ([download link](https://syncandshare.lrz.de/dl/fiLLqkiVLXGQPnrj7UvLtBbN)) orginal raw data: `reviews_subset.json`, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = np.load('top_25k_words.npy')\n",
    "bow_subset = sp.load_npz('bow_subset.npz')\n",
    "N = bow_subset.shape[0]\n",
    "\n",
    "with open('reviews_subset.json' , 'r') as f:\n",
    "    reviews = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The size of the data matrix should be $100K \\times 25K$. Meaning we have $100K$ reviews, and each review is represented as bag-of-words vector of size $25K$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 25000)\n"
     ]
    }
   ],
   "source": [
    "print(bow_subset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the $25K$ we can see which word is associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['food' 'good' 'place' ..., \"burke's\" 'electronica' 'peels']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also inspect how the raw data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'biz': {'categories': ['Restaurants', 'Steakhouses'],\n",
       "  'city': 'Las Vegas',\n",
       "  'name': \"Gallagher's Steakhouse\"},\n",
       " 'business_id': 'nnDLapJk1z2NJE0-XzwMlQ',\n",
       " 'review_id': '0Lx-agcwqjNhS4JtCVqkHQ',\n",
       " 'stars': 5,\n",
       " 'text': \"The surf and turf here was one of the best I've had.\\n\\nFilet mignon and lobster tail was very good.  i generally dont think the food in Vegas is great, but after being dissappointed on many occasions, I was pleasantly surprised with the quality of our meal.  Thanks to the Taste, i was lured to this restaurant only to find that it is a somehat hidden jewel in New York New York close to the sometimes obnoxious piana bar time square.  \\n\\nThe side of green beans were delish and the potatos are just meh.\\n\\nFor desert they  served an extremely delicious lemon pudding which taste more tart than pudding....it was fabulous.\\n\\nI think Gallaghers is good for couples, groups, and even kids.\",\n",
       " 'user_id': 'M63uPVZtCv7ReY2RgJRmOw'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementaion\n",
    "\n",
    "Your task is to implement duplicate detection using LSH with cosine similarity.\n",
    "More specifically you have to:\n",
    "* Generate duplicate **candidates** based on LSH with $b$ bands and $r$ rows per band\n",
    "* Refine the candidates by computing the exact cosine distance\n",
    "* Report all pairs/duplicates with cosine distance < $d$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide a helper function that computes the cosine distance between two rows of a given sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_distance(X, i, j):\n",
    "    \"\"\"Compute cosine distance between two rows of a sparse matrix.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : sp.spmatrix, shape [N, D]\n",
    "        Sparse data matrix.\n",
    "    i : int\n",
    "        Index of the first row.\n",
    "    j : int\n",
    "        Index of the second row.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    d : float\n",
    "        Cosine distance between the two rows of the sparse matrix.\n",
    "        \n",
    "    \"\"\"\n",
    "    i_norm = spl.norm(X[i])\n",
    "    j_norm = spl.norm(X[j])\n",
    "    ij_dot = X[i].dot(X[j].T)[0, 0]\n",
    "    \n",
    "    return 1-ij_dot/(i_norm*j_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LSH(X, b=8, r=32, d=0.3):\n",
    "    \"\"\"Find candidate duplicate pairs using LSH and refine using exact cosine distance.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : sp.spmatrix, shape [N, D]\n",
    "        Sparse data matrix.\n",
    "    b : int\n",
    "        Number of bands.\n",
    "    r : int\n",
    "        Number of rows per band.\n",
    "    d : float\n",
    "        Distance treshold for reporting duplicates.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    duplicates : {(ID1, ID2, d_{12}), ..., (IDX, IDY, d_{xy})}\n",
    "        A set of tuples indicating the detected duplicates.\n",
    "        Each tuple should have 3 elements:\n",
    "            * ID of the first review\n",
    "            * ID of the second review\n",
    "            * The cosine distance between them\n",
    "    \n",
    "    n_candidates : int\n",
    "        Number of detected candidate pairs.\n",
    "        \n",
    "    \"\"\"\n",
    "    np.random.seed(158)\n",
    "    n_candidates = 0\n",
    "    duplicates = set()\n",
    "\n",
    "    ### YOUR CODE HERE ###\n",
    "            \n",
    "    return duplicates, n_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "duplicates, n_candidates = LSH(bow_subset, b=6, r=28, d=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We detected 861 candidates.\n"
     ]
    }
   ],
   "source": [
    "print('We detected {} candidates.'.format(n_candidates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the duplicates we have found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1809, 13244, 0.0),\n",
       " (6600, 93310, 0.0),\n",
       " (17779, 72702, 1.1102230246251565e-16),\n",
       " (32066, 71726, 0.0),\n",
       " (35163, 82597, 1.1102230246251565e-16),\n",
       " (42795, 95179, 0.0),\n",
       " (47940, 65607, 0.0),\n",
       " (64525, 70002, 0.12712843905603044)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the raw data for the reviews that were detected as duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "similar_review_pairs = []\n",
    "for ix1, ix2, sim in duplicates:\n",
    "    rev1 = reviews[ix1]\n",
    "    rev2 = reviews[ix2]\n",
    "    similar_review_pairs.append((rev1, rev2, sim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the all the detected pairs that are not exact duplicates, highlighting the words that are present in our vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pattern = re.compile('(\\S+|\\n)')\n",
    "pattern_punctuation = re.compile('^\\W+|\\W+$')\n",
    "\n",
    "def highlight_vocabulary(text, vocabulary):\n",
    "    split = re.findall(r\"[\\w']+\", text)\n",
    "    in_vocabulary = [pattern.sub(lambda m: pattern_punctuation.sub('', m.group()), str.lower(x)) in words for x in split]\n",
    "    highlighted_words = [\"**{}**\".format(x) if in_vocabulary[ix] else x for ix,x in enumerate(split) ]\n",
    "    highlighted_text = \" \".join(highlighted_words)\n",
    "    return highlighted_text\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Good** **Service** **Good** **food**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "vs.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Service** is **good** very **friendly** and **good** **food** who **could** **want** more nagar it was very **good**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===\n",
      "Same business: False\n",
      "Same user: False\n",
      "Cosine distance 0.1271\n",
      "\n",
      "#######\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for r1, r2, sim in similar_review_pairs:\n",
    "    if not np.isclose(sim, 0.0):\n",
    "        printmd(highlight_vocabulary(r1['text'], words))\n",
    "        print(\"\")\n",
    "        print(\"vs.\")\n",
    "        print(\"\")\n",
    "        printmd(highlight_vocabulary(r2['text'], words))\n",
    "        print(\"===\")\n",
    "        print(\"Same business: {}\".format(r1['business_id'] == r2['business_id']))\n",
    "        print(\"Same user: {}\".format(r1['user_id'] == r2['user_id']))\n",
    "        print(\"Cosine distance {:.4f}\".format(sim))\n",
    "        print(\"\")    \n",
    "        print(\"#######\")\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Intractability (Optional)\n",
    "\n",
    "You can optionally verify for yourself that a naive distance computation is not feasible on a dataset of this size.\n",
    "\n",
    "For example, we get an out of memory error on a machine with 64GB RAM. \n",
    "\n",
    "**Be careful, if you do decide to uncomment and run the code in the next cell your machine may freeze.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics.pairwise import pairwise_distances\n",
    "# distances = pairwise_distances(bow_subset, metric='cosine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can potentially deal with the out of memory error by examining the pairs sequentially and saving only the potential candidates. This would take $O(N^2)$ time.\n",
    "\n",
    "Optionally uncomment the code in the following cell and compare the runtime with your $O(N)$ LSH implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # naively compute the duplicates using a double for loop\n",
    "#\n",
    "# d = 0.2  # treshold for reporting duplicates\n",
    "# duplicates = set()\n",
    "# for i in range(N):\n",
    "#     for j in range(N):\n",
    "#         d_ij = cosine_distance(bow_subset, i, j)\n",
    "#         if d_ij < d and i != j:\n",
    "#             duplicates.add((i, j, d_ij))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
