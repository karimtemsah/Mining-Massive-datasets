{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project task 01: Duplicate detection with LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "\n",
    "from scipy.sparse import linalg as spl\n",
    "from collections import defaultdict\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this task is to find duplicate reviews in the Yelp dataset. You can imagine a scenario were the **same** review appears for a restaurant with only small variation (e.g. few words are changed), or some user is trying to submit spam reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be working with reviews for restaurants in Las Vegas. \n",
    "\n",
    "The data includes:\n",
    "* ([download link](https://syncandshare.lrz.de/dl/fiUuDugamX2JCH33hkxYXgdF)) the preprocessed data matrix: `bow_subset.npz`, \n",
    "* ([download link](https://syncandshare.lrz.de/dl/fiMX6taWPMEQ9aaznq4oadyq)) the words in our vocabulary: `top_25k_words.npy`, \n",
    "* ([download link](https://syncandshare.lrz.de/dl/fiLLqkiVLXGQPnrj7UvLtBbN)) orginal raw data: `reviews_subset.json`, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = np.load('top_25k_words.npy')\n",
    "bow_subset = sp.load_npz('bow_subset.npz')\n",
    "N = bow_subset.shape[0]\n",
    "\n",
    "with open('reviews_subset.json' , 'r') as f:\n",
    "    reviews = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The size of the data matrix should be $100K \\times 25K$. Meaning we have $100K$ reviews, and each review is represented as bag-of-words vector of size $25K$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 25000)\n"
     ]
    }
   ],
   "source": [
    "print(bow_subset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the $25K$ we can see which word is associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['food' 'good' 'place' ..., \"burke's\" 'electronica' 'peels']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also inspect how the raw data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'biz': {'categories': ['Restaurants', 'Steakhouses'],\n",
       "  'city': 'Las Vegas',\n",
       "  'name': \"Gallagher's Steakhouse\"},\n",
       " 'business_id': 'nnDLapJk1z2NJE0-XzwMlQ',\n",
       " 'review_id': '0Lx-agcwqjNhS4JtCVqkHQ',\n",
       " 'stars': 5,\n",
       " 'text': \"The surf and turf here was one of the best I've had.\\n\\nFilet mignon and lobster tail was very good.  i generally dont think the food in Vegas is great, but after being dissappointed on many occasions, I was pleasantly surprised with the quality of our meal.  Thanks to the Taste, i was lured to this restaurant only to find that it is a somehat hidden jewel in New York New York close to the sometimes obnoxious piana bar time square.  \\n\\nThe side of green beans were delish and the potatos are just meh.\\n\\nFor desert they  served an extremely delicious lemon pudding which taste more tart than pudding....it was fabulous.\\n\\nI think Gallaghers is good for couples, groups, and even kids.\",\n",
       " 'user_id': 'M63uPVZtCv7ReY2RgJRmOw'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementaion\n",
    "\n",
    "Your task is to implement duplicate detection using LSH with cosine similarity.\n",
    "More specifically you have to:\n",
    "* Generate duplicate **candidates** based on LSH with $b$ bands and $r$ rows per band\n",
    "* Refine the candidates by computing the exact cosine distance\n",
    "* Report all pairs/duplicates with cosine distance < $d$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide a helper function that computes the cosine distance between two rows of a given sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_distance(X, i, j):\n",
    "    \"\"\"Compute cosine distance between two rows of a sparse matrix.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : sp.spmatrix, shape [N, D]\n",
    "        Sparse data matrix.\n",
    "    i : int\n",
    "        Index of the first row.\n",
    "    j : int\n",
    "        Index of the second row.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    d : float\n",
    "        Cosine distance between the two rows of the sparse matrix.\n",
    "        \n",
    "    \"\"\"\n",
    "    i_norm = spl.norm(X[i])\n",
    "    j_norm = spl.norm(X[j])\n",
    "    ij_dot = X[i].dot(X[j].T)[0, 0]\n",
    "    \n",
    "    return 1-ij_dot/(i_norm*j_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def make_planes(size, dim, seed):\n",
    "    np.random.seed(seed)\n",
    "    p = []\n",
    "    for i in range(size):\n",
    "        p.append(np.random.normal(0, 1, dim).tolist())\n",
    "        #p.append(np.random.uniform(-1, 1, dim).tolist())\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LSH(X, b=8, r=32, d=0.3):\n",
    "    \"\"\"Find candidate duplicate pairs using LSH and refine using exact cosine distance.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : sp.spmatrix, shape [N, D]\n",
    "        Sparse data matrix.\n",
    "    b : int\n",
    "        Number of bands.\n",
    "    r : int\n",
    "        Number of rows per band.\n",
    "    d : float\n",
    "        Distance treshold for reporting duplicates.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    duplicates : {(ID1, ID2, d_{12}), ..., (IDX, IDY, d_{xy})}\n",
    "        A set of tuples indicating the detected duplicates.\n",
    "        Each tuple should have 3 elements:\n",
    "            * ID of the first review\n",
    "            * ID of the second review\n",
    "            * The cosine distance between them\n",
    "    \n",
    "    n_candidates : int\n",
    "        Number of detected candidate pairs.\n",
    "        \n",
    "    \"\"\"\n",
    "    np.random.seed(158)\n",
    "    n_candidates = 0\n",
    "    duplicates = set()\n",
    "    \n",
    "    ### YOUR CODE HERE ###\n",
    "    ### Genrating a signature matrix###\n",
    "    planes = make_planes(size=b*r,dim=X.shape[1],seed=158)\n",
    "    planes = np.asmatrix(planes)\n",
    "    sketch = planes * X.transpose()\n",
    "    sketch[sketch > 0] = 1\n",
    "    sketch[sketch <= 0] = 0\n",
    "    detectedPairs = {}\n",
    "    dic = defaultdict(list)\n",
    "    for x in range(b):  \n",
    "        for y in range(sketch.shape[1]):\n",
    "            reviewValues = sketch[(x*r):(r*(x+1)),[y]]\n",
    "            reviewValues = reviewValues.transpose().tolist()\n",
    "            binaryReview = ''.join(str(int(v)) for v in reviewValues[0])\n",
    "            reviewHashValue = int(binaryReview, 2)\n",
    "            dic['{}{}'.format(reviewHashValue, x)].append(y)\n",
    "          \n",
    "    for k,v in dic.items(): \n",
    "        if(len(v) > 1):\n",
    "            allCandidates = list(itertools.combinations(v, 2))\n",
    "            for pair in allCandidates:\n",
    "                if not '{}{}'.format(pair[0], pair[1]) in detectedPairs.keys():\n",
    "                    detectedPairs['{}{}'.format(pair[0], pair[1])] = 1\n",
    "                    n_candidates = n_candidates + 1\n",
    "                    d_ij = cosine_distance(X,pair[0],pair[1])\n",
    "                    if(d_ij < d and pair[0] != pair[1]):\n",
    "                        duplicates.add((pair[0], pair[1], d_ij))\n",
    "\n",
    "                        \n",
    "    \n",
    "    return duplicates, n_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates, n_candidates = LSH(bow_subset, b=6, r=28, d=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We detected 826 candidates.\n"
     ]
    }
   ],
   "source": [
    "print('We detected {} candidates.'.format(n_candidates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the duplicates we have found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1809, 13244, 0.0),\n",
       " (6600, 93310, 0.0),\n",
       " (17779, 72702, 1.1102230246251565e-16),\n",
       " (32066, 71726, 0.0),\n",
       " (35163, 82597, 1.1102230246251565e-16),\n",
       " (42795, 95179, 0.0),\n",
       " (47940, 65607, 0.0),\n",
       " (64525, 70002, 0.12712843905603044)}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the raw data for the reviews that were detected as duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "similar_review_pairs = []\n",
    "for ix1, ix2, sim in duplicates:\n",
    "    rev1 = reviews[ix1]\n",
    "    rev2 = reviews[ix2]\n",
    "    similar_review_pairs.append((rev1, rev2, sim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the all the detected pairs that are not exact duplicates, highlighting the words that are present in our vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pattern = re.compile('(\\S+|\\n)')\n",
    "pattern_punctuation = re.compile('^\\W+|\\W+$')\n",
    "\n",
    "def highlight_vocabulary(text, vocabulary):\n",
    "    split = re.findall(r\"[\\w']+\", text)\n",
    "    in_vocabulary = [pattern.sub(lambda m: pattern_punctuation.sub('', m.group()), str.lower(x)) in words for x in split]\n",
    "    highlighted_words = [\"**{}**\".format(x) if in_vocabulary[ix] else x for ix,x in enumerate(split) ]\n",
    "    highlighted_text = \" \".join(highlighted_words)\n",
    "    return highlighted_text\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Good** **Service** **Good** **food**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "vs.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Service** is **good** very **friendly** and **good** **food** who **could** **want** more nagar it was very **good**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===\n",
      "Same business: False\n",
      "Same user: False\n",
      "Cosine distance 0.1271\n",
      "\n",
      "#######\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for r1, r2, sim in similar_review_pairs:\n",
    "    if not np.isclose(sim, 0.0):\n",
    "        printmd(highlight_vocabulary(r1['text'], words))\n",
    "        print(\"\")\n",
    "        print(\"vs.\")\n",
    "        print(\"\")\n",
    "        printmd(highlight_vocabulary(r2['text'], words))\n",
    "        print(\"===\")\n",
    "        print(\"Same business: {}\".format(r1['business_id'] == r2['business_id']))\n",
    "        print(\"Same user: {}\".format(r1['user_id'] == r2['user_id']))\n",
    "        print(\"Cosine distance {:.4f}\".format(sim))\n",
    "        print(\"\")    \n",
    "        print(\"#######\")\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Intractability (Optional)\n",
    "\n",
    "You can optionally verify for yourself that a naive distance computation is not feasible on a dataset of this size.\n",
    "\n",
    "For example, we get an out of memory error on a machine with 64GB RAM. \n",
    "\n",
    "**Be careful, if you do decide to uncomment and run the code in the next cell your machine may freeze.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics.pairwise import pairwise_distances\n",
    "# distances = pairwise_distances(bow_subset, metric='cosine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can potentially deal with the out of memory error by examining the pairs sequentially and saving only the potential candidates. This would take $O(N^2)$ time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# naively compute the duplicates using a double for loop\n",
    "def naive_duplicates(X, d = 0.2):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : sp.spmatrix, shape [N, D]\n",
    "        Sparse data matrix.\n",
    "    d : float\n",
    "        Distance treshold for reporting duplicates.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    duplicates : {(ID1, ID2, d_{12}), ..., (IDX, IDY, d_{xy})}\n",
    "        A set of tuples indicating the detected duplicates.\n",
    "        Each tuple should have 3 elements:\n",
    "            * ID of the first review\n",
    "            * ID of the second review\n",
    "            * The cosine distance between them\n",
    "    \"\"\"\n",
    "    N = X.shape[0]\n",
    "    duplicates = set()\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            d_ij = cosine_distance(X, i, j)\n",
    "            if d_ij < d and i != j:\n",
    "                duplicates.add((i, j, d_ij))\n",
    "    return duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to implement code for runtime comparison between LSH and the naive nested for loop implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runtime_comparison():\n",
    "    \"\"\"\n",
    "    Compare the runtime between LSH and the naive approach.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    trace : [(n1, lsh_dur, naive_dur), (n2, lsh_dur, naive_dur), ... ]\n",
    "            A list of tuples with execution times for different number of reviews.\n",
    "            Each tuple should have 3 elements:\n",
    "                * number of reviews considered\n",
    "                * duration of the LSH approach\n",
    "                * duration of the naive approach\n",
    "    \"\"\"\n",
    "    trace = []\n",
    "    for n in np.arange(25, 251, 25):\n",
    "        print('Running comparison for {} reviews.'.format(n))\n",
    "        ### YOUR CODE HERE ###\n",
    "        subSet = bow_subset[0:n,:]\n",
    "        start = time.clock()\n",
    "        LSH(bow_subset[0:n,:], b=6, r=28, d=0.2)\n",
    "        end = time.clock()\n",
    "        timeLSH = end - start\n",
    "        start = time.clock()\n",
    "        naive_duplicates(subSet, d = 0.2)\n",
    "        end = time.clock()\n",
    "        timeNaive = end - start\n",
    "        trace.append((n,timeLSH,timeNaive))\n",
    "        \n",
    "    return trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running comparison for 25 reviews.\n",
      "Running comparison for 50 reviews.\n",
      "Running comparison for 75 reviews.\n",
      "Running comparison for 100 reviews.\n",
      "Running comparison for 125 reviews.\n",
      "Running comparison for 150 reviews.\n",
      "Running comparison for 175 reviews.\n",
      "Running comparison for 200 reviews.\n",
      "Running comparison for 225 reviews.\n",
      "Running comparison for 250 reviews.\n"
     ]
    }
   ],
   "source": [
    "trace = runtime_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the differecene in runtime. On the x-axis plot the number of reviews processed and on the y-axis plot the runtime in seconds for both approaches. You should obtain a plot similar to the one shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### YOUR PLOTTING CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x, y1, y2 = zip(*trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y1,label = \"LSH\")\n",
    "plt.plot(x,y2,label = \"Naive\")\n",
    "plt.legend()\n",
    "plt.title('Runtime Comparison')\n",
    "plt.xlabel('Number of reviews processed')\n",
    "plt.ylabel('Time in Seconds')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
