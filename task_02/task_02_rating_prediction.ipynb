{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project task 02: Restaurant recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "from scipy.sparse.linalg import svds\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "import time\n",
    "import random, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/anaconda3/envs/mmds_virtual/bin/python'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this task is to recommend restaurants to users based on the rating data in the Yelp dataset. For this, we try to predict the rating a user will give to a restaurant they have not been to yet based on a latent factor model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First download `ratings.npy` from Piazza ([download link](https://syncandshare.lrz.de/dl/fiKMoxRNusLoFpFHkXXEgvdZ/ratings.npy))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = np.load(\"ratings.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[101968,   1880,      1],\n",
       "       [101968,    284,      5],\n",
       "       [101968,   1378,      2],\n",
       "       ...,\n",
       "       [ 72452,   2100,      4],\n",
       "       [ 72452,   2050,      5],\n",
       "       [ 74861,   3979,      5]], dtype=uint32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have triplets of (user, restaurant, rating).\n",
    "ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we transform the data into a matrix of dimension [N, D], where N is the number of users and D is the number of restaurants in the dataset.  \n",
    "We **strongly recommend** to load the data as a sparse matrix to avoid out-of-memory issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<337867x5899 sparse matrix of type '<class 'numpy.uint32'>'\n",
       "\twith 929606 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_users = len(np.unique(ratings[:,0]))\n",
    "number_of_restaurants = len(np.unique(ratings[:,1]))\n",
    "M = sp.csr_matrix((ratings[:,2], (ratings[:,0], ratings[:,1])), \n",
    "                  shape=(number_of_users, number_of_restaurants))\n",
    "M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preprocessing step, we recursively remove all users and restaurants with 10 or less ratings. \n",
    "\n",
    "Then, we randomly select 200 data points for the validation and test sets, respectively.\n",
    "\n",
    "After this, we subtract the mean rating for each users to account for this global effect.   \n",
    "**Hint**: Some entries might become zero in this process -- but these entries are different than the 'unknown' zeros in the matrix. Store the indices of which we have data available in a separate variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cold_start_preprocessing(matrix, min_entries):\n",
    "    \"\"\"\n",
    "    Recursively removes rows and columns from the input matrix which have less than min_entries nonzero entries.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix      : sp.spmatrix, shape [N, D]\n",
    "                  The input matrix to be preprocessed.\n",
    "    min_entries : int\n",
    "                  Minimum number of nonzero elements per row and column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    matrix      : sp.spmatrix, shape [N', D']\n",
    "                  The pre-processed matrix, where N' <= N and D' <= D\n",
    "        \n",
    "    \"\"\"\n",
    "    print(\"Shape before: {}\".format(matrix.shape))\n",
    "    \n",
    "    ### YOUR CODE HERE ###\n",
    "    while True:\n",
    "        shape = matrix.shape\n",
    "        matrix = matrix[matrix.getnnz(1)>min_entries]\n",
    "        matrix = matrix[:,matrix.getnnz(0)>min_entries]\n",
    "        shape_new = matrix.shape\n",
    "        if shape == shape_new:\n",
    "            break\n",
    "    print(\"Shape after: {}\".format(matrix.shape))\n",
    "    nnz = matrix>0\n",
    "    assert (nnz.sum(0).A1 > min_entries).all()\n",
    "    assert (nnz.sum(1).A1 > min_entries).all()\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_user_mean(matrix):\n",
    "    \"\"\"\n",
    "    Subtract the mean rating per user from the non-zero elements in the input matrix.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix : sp.spmatrix, shape [N, D]\n",
    "             Input sparse matrix.\n",
    "    Returns\n",
    "    -------\n",
    "    matrix : sp.spmatrix, shape [N, D]\n",
    "             The modified input matrix.\n",
    "    \n",
    "    user_means : np.array, shape [N, 1]\n",
    "                 The mean rating per user that can be used to recover the absolute ratings from the mean-shifted ones.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    ### YOUR CODE HERE ###\n",
    "    tot = np.array(matrix.sum(axis=1).squeeze())[0]\n",
    "    cts = np.diff(matrix.indptr)\n",
    "    user_means = tot/cts\n",
    "    d = sp.diags(user_means, 0)\n",
    "    b = matrix.copy()\n",
    "    b.data = np.ones_like(b.data)\n",
    "    matrix = (matrix - d*b)   \n",
    "    assert np.all(np.isclose(matrix.mean(1), 0))\n",
    "    return matrix, user_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(matrix, n_validation, n_test):\n",
    "    \"\"\"\n",
    "    Extract validation and test entries from the input matrix. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix          : sp.spmatrix, shape [N, D]\n",
    "                      The input data matrix.\n",
    "    n_validation    : int\n",
    "                      The number of validation entries to extract.\n",
    "    n_test          : int\n",
    "                      The number of test entries to extract.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    matrix_split    : sp.spmatrix, shape [N, D]\n",
    "                      A copy of the input matrix in which the validation and test entries have been set to zero.\n",
    "    \n",
    "    val_idx         : tuple, shape [2, n_validation]\n",
    "                      The indices of the validation entries.\n",
    "    \n",
    "    test_idx        : tuple, shape [2, n_test]\n",
    "                      The indices of the test entries.\n",
    "    \n",
    "    val_values      : np.array, shape [n_validation, ]\n",
    "                      The values of the input matrix at the validation indices.\n",
    "                      \n",
    "    test_values     : np.array, shape [n_test, ]\n",
    "                      The values of the input matrix at the test indices.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    ### YOUR CODE HERE ###\n",
    "    indices = matrix.nonzero()\n",
    "    a = np.arange(0, len(indices[0]), 1)\n",
    "    np.random.shuffle(a)\n",
    "    val_idx = ([],[])\n",
    "    test_idx = ([],[])\n",
    "    val_values = np.zeros(n_validation)\n",
    "    test_values = np.zeros(n_test)\n",
    "    matrix_split = matrix.copy()\n",
    "\n",
    "\n",
    "    for i in range(n_validation):\n",
    "            val_idx[0].append(indices[0][a[i]])\n",
    "            val_idx[1].append(indices[1][a[i]])\n",
    "            val_values[i] = matrix[indices[0][a[i]],indices[1][a[i]]]\n",
    "            matrix_split[indices[0][a[i]],indices[1][a[i]]] = 0\n",
    "            \n",
    "            \n",
    "    for i in range(n_test):\n",
    "            test_idx[0].append(indices[0][a[i + n_validation]])\n",
    "            test_idx[1].append(indices[1][a[i + n_validation]])\n",
    "            test_values[i] = matrix[indices[0][a[i + n_validation]],indices[1][a[i + n_validation]]]\n",
    "            matrix_split[indices[0][a[i + n_validation]],indices[1][a[i + n_validation]]] = 0\n",
    "    \n",
    "    matrix_split.eliminate_zeros()\n",
    "    return matrix_split, val_idx, test_idx, val_values, test_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before: (337867, 5899)\n",
      "Shape after: (11275, 3531)\n"
     ]
    }
   ],
   "source": [
    "M = cold_start_preprocessing(M, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_validation = 200\n",
    "n_test = 200\n",
    "# Split data\n",
    "M_train, val_idx, test_idx, val_values, test_values = split_data(M, n_validation, n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store away the nonzero indices of M before subtracting the row means.\n",
    "nonzero_indices = M_train.nonzero()\n",
    "\n",
    "# Remove user means.\n",
    "M_shifted, user_means = shift_user_mean(M_train)\n",
    "\n",
    "# Apply the same shift to the validation and test data.\n",
    "val_values_shifted = val_values.copy()\n",
    "test_values_shifted = test_values.copy()\n",
    "for i in range(0, n_validation):\n",
    "    val_values_shifted[i] = val_values_shifted[i] - user_means[val_idx[0][i]]\n",
    "    test_values_shifted[i] = test_values_shifted[i] - user_means[test_idx[0][i]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Alternating optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first step, we will approach the problem via alternating optimization, as learned in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_Q_P(matrix, k, init='random'):\n",
    "    \"\"\"\n",
    "    Initialize the matrices Q and P for a latent factor model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix : sp.spmatrix, shape [N, D]\n",
    "             The matrix to be factorized.\n",
    "    k      : int\n",
    "             The number of latent dimensions.\n",
    "    init   : str in ['svd', 'random'], default: 'random'\n",
    "             The initialization strategy. 'svd' means that we use SVD to initialize P and Q, 'random' means we initialize\n",
    "             the entries in P and Q randomly in the interval [0, 1).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Q : np.array, shape [N, k]\n",
    "        The initialized matrix Q of a latent factor model.\n",
    "\n",
    "    P : np.array, shape [k, D]\n",
    "        The initialized matrix P of a latent factor model.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    if init == 'svd':\n",
    "        U, sigma, P = svds(matrix, k)\n",
    "        x = np.eye(k)\n",
    "        sigma = x * sigma\n",
    "        Q = np.matmul(U, sigma)\n",
    "    elif init == 'random':\n",
    "        Q = np.random.rand(matrix.shape[0], k)\n",
    "        P = np.random.rand(k, matrix.shape[1])\n",
    "    else:\n",
    "        raise ValueError\n",
    "        \n",
    "    assert Q.shape == (matrix.shape[0], k)\n",
    "    assert P.shape == (k, matrix.shape[1])\n",
    "    return Q, P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, P = initialize_Q_P(M_shifted, 30, init='svd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latent_factor_alternating_optimization(M, non_zero_idx, k, val_idx, val_values,\n",
    "                                           reg_lambda, max_steps=100, init='random',\n",
    "                                           log_every=1, patience=10, eval_every=1):\n",
    "    \"\"\"\n",
    "    Perform matrix factorization using alternating optimization. Training is done via patience,\n",
    "    i.e. we stop training after we observe no improvement on the validation loss for a certain\n",
    "    amount of training steps. We then return the best values for Q and P oberved during training.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    M                 : sp.spmatrix, shape [N, D]\n",
    "                        The input matrix to be factorized.\n",
    "                      \n",
    "    non_zero_idx      : np.array, shape [nnz, 2]\n",
    "                        The indices of the non-zero entries of the un-shifted matrix to be factorized. \n",
    "                        nnz refers to the number of non-zero entries. Note that this may be different\n",
    "                        from the number of non-zero entries in the input matrix M, e.g. in the case\n",
    "                        that all ratings by a user have the same value.\n",
    "    \n",
    "    k                 : int\n",
    "                        The latent factor dimension.\n",
    "    \n",
    "    val_idx           : tuple, shape [2, n_validation]\n",
    "                        Tuple of the validation set indices.\n",
    "                        n_validation refers to the size of the validation set.\n",
    "                      \n",
    "    val_values        : np.array, shape [n_validation, ]\n",
    "                        The values in the validation set.\n",
    "                      \n",
    "    reg_lambda        : float\n",
    "                        The regularization strength.\n",
    "                      \n",
    "    max_steps         : int, optional, default: 100\n",
    "                        Maximum number of training steps. Note that we will stop early if we observe\n",
    "                        no improvement on the validation error for a specified number of steps\n",
    "                        (see \"patience\" for details).\n",
    "                      \n",
    "    init              : str in ['random', 'svd'], default 'random'\n",
    "                        The initialization strategy for P and Q. See function initialize_Q_P for details.\n",
    "    \n",
    "    log_every         : int, optional, default: 1\n",
    "                        Log the training status every X iterations.\n",
    "                    \n",
    "    patience          : int, optional, default: 10\n",
    "                        Stop training after we observe no improvement of the validation loss for X evaluation\n",
    "                        iterations (see eval_every for details). After we stop training, we restore the best \n",
    "                        observed values for Q and P (based on the validation loss) and return them.\n",
    "                      \n",
    "    eval_every        : int, optional, default: 1\n",
    "                        Evaluate the training and validation loss every X steps. If we observe no improvement\n",
    "                        of the validation error, we decrease our patience by 1, else we reset it to *patience*.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    best_Q            : np.array, shape [N, k]\n",
    "                        Best value for Q (based on validation loss) observed during training\n",
    "                      \n",
    "    best_P            : np.array, shape [k, D]\n",
    "                        Best value for P (based on validation loss) observed during training\n",
    "                      \n",
    "    validation_losses : list of floats\n",
    "                        Validation loss for every evaluation iteration, can be used for plotting the validation\n",
    "                        loss over time.\n",
    "                        \n",
    "    train_losses      : list of floats\n",
    "                        Training loss for every evaluation iteration, can be used for plotting the training\n",
    "                        loss over time.                     \n",
    "    \n",
    "    converged_after   : int\n",
    "                        it - patience*eval_every, where it is the iteration in which patience hits 0,\n",
    "                        or -1 if we hit max_steps before converging. \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE ###\n",
    "    \n",
    "    temp_patience = patience\n",
    "    initial_val_loss = 10000000000\n",
    "    train_losses = []\n",
    "    validation_losses = []\n",
    "    Q, P = initialize_Q_P(M, k, init)\n",
    "    for step in range(max_steps):\n",
    "        i = non_zero_idx[0]\n",
    "        j = non_zero_idx[1]\n",
    "        num_user = M.shape[0]\n",
    "        num_items = M.shape[1]\n",
    "        for u in range(num_items):\n",
    "            clf = Ridge(alpha = reg_lambda)\n",
    "            MM = M[:, u]\n",
    "            PP = Q\n",
    "            clf.fit(X = PP, y= MM.todense().A1)\n",
    "            P[:,u] = clf.coef_\n",
    "        for n in range(num_user):\n",
    "            clf = Ridge(alpha = reg_lambda)\n",
    "            MM = M[n, :]\n",
    "            PP = P.transpose()\n",
    "            clf.fit(X = PP, y= MM.todense().A1)\n",
    "            Q[n, :] = clf.coef_\n",
    "\n",
    "        print(P.shape)\n",
    "        print(Q.shape)\n",
    "        if step % eval_every == 0:\n",
    "            # calculate the training loss\n",
    "            train_loss = 0\n",
    "            validation_loss = 0\n",
    "            i = non_zero_idx[0]\n",
    "            j = non_zero_idx[1]\n",
    "            train_loss = np.sum(np.square(M[i,j] - np.matmul(Q,P)[i,j]))\n",
    "            # calculate the validation loss\n",
    "            for count in range(len(val_values)):\n",
    "                i = val_idx[0][count]\n",
    "                j = val_idx[1][count]\n",
    "                validation_loss += pow(val_values[count] - np.dot(Q[i,:], P[:,j]), 2)    \n",
    "                validation_loss += reg_lambda * (np.sum(pow(Q[i, :], 2)) + np.sum(pow(P[:, j],2)))\n",
    "            if validation_loss < initial_val_loss:\n",
    "                initial_val_loss = validation_loss\n",
    "                best_Q = Q\n",
    "                best_P = P\n",
    "            if len(validation_losses) > 0:\n",
    "                if abs(validation_loss - validation_losses[-1]) < 1:\n",
    "                    temp_patience -= 1\n",
    "                else:\n",
    "                    temp_patience = patience\n",
    "            train_losses.append(train_loss)\n",
    "            validation_losses.append(validation_loss)\n",
    "            if temp_patience == 0:\n",
    "                converged_after = step - patience * eval_every\n",
    "                print('Converged after {} iterations'.format(converged_after))\n",
    "                break\n",
    "        if step % log_every == 0:\n",
    "            print ('Iteration {}, training loss: {}, validation loss: {}'.format(step, train_loss, validation_loss))\n",
    "            \n",
    "    \n",
    "    return best_Q, best_P, validation_losses, train_losses, converged_after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the latent factor model with alternating optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Learn the optimal $P$ and $Q$ using alternating optimization. That is, during each iteration you first update $Q$ while having $P$ fixed and then vice versa. Run the alternating optimization algorithm with $k=100$ and $\\lambda=1$. Plot the training and validation losses over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_a, P_a, val_l_a, tr_l_a, conv_a = latent_factor_alternating_optimization(M_shifted, nonzero_indices, \n",
    "                                                                           k=100, val_idx=val_idx,\n",
    "                                                                           val_values=val_values_shifted, \n",
    "                                                                           reg_lambda=1, init='random',\n",
    "                                                                           max_steps=100, patience=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the validation and training losses over (training) time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### YOUR PLOTTING CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) (**Optional**): Try some different latent dimensions $k$ in the range [5, 100]. What do you observe (convergence time, final training/validation losses)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_a_2, P_a_2, val_l_a_2, tr_l_a_2, conv_a_2 = latent_factor_alternating_optimization(M_shifted, nonzero_indices, \n",
    "                                                                           k=20, val_idx=val_idx,\n",
    "                                                                           val_values=val_values_shifted, \n",
    "                                                                           reg_lambda=0.1, init='random',\n",
    "                                                                           max_steps=100, patience=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the validation and training losses over (training) time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### YOUR PLOTTING CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Latent factorization using gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use gradient descent to factorize our ratings matrix. We will try both (mini-) batch and stochastic gradient descent. You can use the following equations for your implementation.\n",
    "\n",
    "Recall that the objective function (loss) we wanted to optimize was:\n",
    "$$\n",
    "\\mathcal{L} = \\min_{P, Q} \\sum_{(x, i) \\in W} (r_{xi} - \\mathbf{q}_i^T\\mathbf{p}_x)^2 + \\lambda_1\\sum_x{\\left\\lVert \\mathbf{p}_x  \\right\\rVert}^2 + \\lambda_2\\sum_i {\\left\\lVert\\mathbf{q}_i  \\right\\rVert}^2\n",
    "$$\n",
    "\n",
    "where $W$ is the set of $(x, i)$ pairs for which $r_{xi}$ is known (in this case our known play counts). Here we have also introduced two regularization terms to help us with overfitting where $\\lambda_1$ and $\\lambda_2$ are hyper-parameters that control the strength of the regularization.\n",
    "\n",
    "Naturally optimizing with gradient descent involves computing the gradient of the loss function $\\mathcal{L}$ w.r.t. to the parameters. To help you solve the task we provide the following:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial ((r_{xi} - \\mathbf{q}_i^T\\mathbf{p}_x)^2)}{\\partial \\mathbf{p}_x} = -2(r_{xi} - \\mathbf{q}_i^T\\mathbf{p}_x)\\mathbf{q}_i\\;, ~~~\n",
    "\\frac{\\partial ((r_{xi} - \\mathbf{q}_i^T\\mathbf{p}_x)^2)}{\\partial \\mathbf{q}_i} = -2(r_{xi} - \\mathbf{q}_i^T\\mathbf{p}_x)\\mathbf{p}_x \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial(\\lambda_1{\\left\\lVert \\mathbf{p}_x \\right\\rVert}^2)}{\\partial \\mathbf{p}_x} = 2 \\lambda_1 \\mathbf{p_x} \\;, ~~~\n",
    "\\frac{\\partial(\\lambda_2{\\left\\lVert \\mathbf{q}_i \\right\\rVert}^2)}{\\partial \\mathbf{q}_i} = 2 \\lambda_2 \\mathbf{q_i}\n",
    "$$\n",
    "\n",
    "**Hint**: You have to carefully consider how to combine the given partial gradients depending\n",
    "on which variants of gradient descent you are using.  \n",
    "**Hint 2**: It may be useful to scale the updates to $P$ and $Q$ by $\\frac{1}{batch\\_size}$ (in the case of full-sweep updates, this would be $\\frac{1}{n\\_users}$ for $Q$ and $\\frac{1}{n\\_restaurants}$ for $P$).\n",
    "\n",
    "\n",
    "For each of the gradients descent variants you try report and compare the following:\n",
    "* How many iterations do you need for convergence.\n",
    "* Plot the loss (y axis) for each iteration (x axis).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latent_factor_gradient_descent(M, non_zero_idx, k, val_idx, val_values, \n",
    "                                   reg_lambda, learning_rate, batch_size=-1,\n",
    "                                   max_steps=50000, init='random',\n",
    "                                   log_every=1000, patience=20,\n",
    "                                   eval_every=50):\n",
    "    \"\"\"\n",
    "    Perform matrix factorization using gradient descent. Training is done via patience,\n",
    "    i.e. we stop training after we observe no improvement on the validation loss for a certain\n",
    "    amount of training steps. We then return the best values for Q and P oberved during training.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    M                 : sp.spmatrix, shape [N, D]\n",
    "                        The input matrix to be factorized.\n",
    "                      \n",
    "    non_zero_idx      : np.array, shape [nnz, 2]\n",
    "                        The indices of the non-zero entries of the un-shifted matrix to be factorized. \n",
    "                        nnz refers to the number of non-zero entries. Note that this may be different\n",
    "                        from the number of non-zero entries in the input matrix M, e.g. in the case\n",
    "                        that all ratings by a user have the same value.\n",
    "    \n",
    "    k                 : int\n",
    "                        The latent factor dimension.\n",
    "    \n",
    "    val_idx           : tuple, shape [2, n_validation]\n",
    "                        Tuple of the validation set indices.\n",
    "                        n_validation refers to the size of the validation set.\n",
    "                      \n",
    "    val_values        : np.array, shape [n_validation, ]\n",
    "                        The values in the validation set.\n",
    "                      \n",
    "    reg_lambda        : float\n",
    "                        The regularization strength.\n",
    "\n",
    "    learning_rate     : float\n",
    "                        Step size of the gradient descent updates.\n",
    "                        \n",
    "    batch_size        : int, optional, default: -1\n",
    "                        (Mini-) batch size. -1 means we perform standard full-sweep gradient descent.\n",
    "                        If the batch size is >0, use mini batches of this given size.\n",
    "                        \n",
    "    max_steps         : int, optional, default: 100\n",
    "                        Maximum number of training steps. Note that we will stop early if we observe\n",
    "                        no improvement on the validation error for a specified number of steps\n",
    "                        (see \"patience\" for details).\n",
    "                      \n",
    "    init              : str in ['random', 'svd'], default 'random'\n",
    "                        The initialization strategy for P and Q. See function initialize_Q_P for details.\n",
    "    \n",
    "    log_every         : int, optional, default: 1\n",
    "                        Log the training status every X iterations.\n",
    "                    \n",
    "    patience          : int, optional, default: 10\n",
    "                        Stop training after we observe no improvement of the validation loss for X evaluation\n",
    "                        iterations (see eval_every for details). After we stop training, we restore the best \n",
    "                        observed values for Q and P (based on the validation loss) and return them.\n",
    "                      \n",
    "    eval_every        : int, optional, default: 1\n",
    "                        Evaluate the training and validation loss every X steps. If we observe no improvement\n",
    "                        of the validation error, we decrease our patience by 1, else we reset it to *patience*.\n",
    "                        \n",
    "    Returns\n",
    "    -------\n",
    "    best_Q            : np.array, shape [N, k]\n",
    "                        Best value for Q (based on validation loss) observed during training\n",
    "                      \n",
    "    best_P            : np.array, shape [k, D]\n",
    "                        Best value for P (based on validation loss) observed during training\n",
    "                      \n",
    "    validation_losses : list of floats\n",
    "                        Validation loss for every evaluation iteration, can be used for plotting the validation\n",
    "                        loss over time.\n",
    "                        \n",
    "    train_losses      : list of floats\n",
    "                        Training loss for every evaluation iteration, can be used for plotting the training\n",
    "                        loss over time.                     \n",
    "    \n",
    "    converged_after   : int\n",
    "                        it - patience*eval_every, where it is the iteration in which patience hits 0,\n",
    "                        or -1 if we hit max_steps before converging. \n",
    "\n",
    "    \"\"\"\n",
    "    temp_patience = patience\n",
    "    initial_val_loss = 1e6\n",
    "    train_losses = []\n",
    "    validation_losses = []\n",
    "    Q, P = initialize_Q_P(M, k, init)\n",
    "    non_zero_matrix = np.matrix((non_zero_idx[0], non_zero_idx[1])).T\n",
    "    number_non_zeros = len(non_zero_idx[0])\n",
    "    start = time.clock()\n",
    "    for step in range(max_steps): \n",
    "        if batch_size != -1:\n",
    "            non_zero_matrix_temp = non_zero_matrix[random.sample(range(0, number_non_zeros), batch_size)]\n",
    "        else:\n",
    "            non_zero_matrix_temp = non_zero_matrix\n",
    "\n",
    "        Q_temp = np.zeros((Q.shape[0], Q.shape[1]))\n",
    "        P_temp = np.zeros((P.shape[0], P.shape[1]))\n",
    "        for count in range(non_zero_matrix_temp.shape[0]):\n",
    "            i = non_zero_matrix_temp[count,0]\n",
    "            j = non_zero_matrix_temp[count,1]\n",
    "            eij = M[i,j] - np.dot(Q[i,:],P[:,j])\n",
    "            Q[i,:] = Q[i,:] + learning_rate * 2 * (eij * P[:,j] - reg_lambda * Q[i,:])\n",
    "            P[:,j] = P[:,j] + learning_rate * 2 * (eij * Q[i,:] - reg_lambda * P[:,j])\n",
    "\n",
    "        if step % eval_every == 0:\n",
    "            # calculate the training loss\n",
    "            train_loss = 0\n",
    "            validation_loss = 0\n",
    "            i = non_zero_idx[0]\n",
    "            j = non_zero_idx[1]\n",
    "            train_loss = np.sum(np.square(M[i,j] - np.matmul(Q,P)[i,j])) + reg_lambda * (np.sum(pow(Q[i, :], 2)) + np.sum(pow(P[:,j],2)))\n",
    "            i = val_idx[0]\n",
    "            j = val_idx[1]\n",
    "            validation_loss = np.sum(np.square(val_values - np.matmul(Q,P)[i,j])) + reg_lambda * (np.sum(pow(Q[i, :], 2)) + np.sum(pow(P[:,j],2)))\n",
    "            if validation_loss < initial_val_loss:\n",
    "                initial_val_loss = validation_loss\n",
    "                best_Q = Q\n",
    "                best_P = P\n",
    "            if len(validation_losses) > 0:\n",
    "                if abs(validation_loss - validation_losses[-1]) < 1:\n",
    "                    temp_patience -= 1\n",
    "                else:\n",
    "                    temp_patience = patience\n",
    "            train_losses.append(train_loss)\n",
    "            validation_losses.append(validation_loss)\n",
    "            if temp_patience == 0:\n",
    "                converged_after = step - patience * eval_every\n",
    "                end = time.clock()\n",
    "                print('Converged after {} iterations, on average {}s per iteration'.format(converged_after, ((end-start)/step)))\n",
    "                break\n",
    "        if step % log_every == 0:\n",
    "            print ('Iteration {}, training loss: {}, validation loss: {}'.format(step, train_loss, validation_loss))\n",
    "            \n",
    "\n",
    "    return best_Q, best_P, validation_losses, train_losses, converged_after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the latent factor model with alternating optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Learn the optimal $P$ and $Q$ using standard gradient descent. That is, during each iteration you have to use all of the training examples and update $Q$ and $P$ for all users and songs at once. Try the algorithm with $k=30$, $\\lambda=1$, and learning rate of 0.1. Initialize $Q$ and $P$ with SVD.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, training loss: 349578.2334373663, validation loss: 249.0824600336538\n",
      "Iteration 10, training loss: 337387.7806997647, validation loss: 240.4936273470866\n",
      "Iteration 20, training loss: 337381.9348144661, validation loss: 240.49354389879343\n",
      "Iteration 30, training loss: 337378.5368741187, validation loss: 240.4935054834346\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-be65f9220b54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                                                                                                    \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'svd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                                                                                    \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                                                                                                    eval_every=10)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-26-1a4bf8131bf0>\u001b[0m in \u001b[0;36mlatent_factor_gradient_descent\u001b[0;34m(M, non_zero_idx, k, val_idx, val_values, reg_lambda, learning_rate, batch_size, max_steps, init, log_every, patience, eval_every)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnon_zero_matrix_temp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnon_zero_matrix_temp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0meij\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m             \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0meij\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mreg_lambda\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0meij\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mreg_lambda\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/mmds_virtual/lib/python3.6/site-packages/scipy/sparse/csr.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0;31m# [i, j]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misintlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_single_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m             \u001b[0;31m# [i, 1:2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/mmds_virtual/lib/python3.6/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36m_get_single_element\u001b[0;34m(self, row, col)\u001b[0m\n\u001b[1;32m    855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindptr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmajor_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 857\u001b[0;31m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindptr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmajor_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_sorted_indices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Q_g_sweep, P_g_sweep, val_l_g_sweep, tr_l_g_sweep, conv_g_sweep =  latent_factor_gradient_descent(M_shifted, nonzero_indices, \n",
    "                                                                                                   k=30, val_idx=val_idx,\n",
    "                                                                                                   val_values=val_values_shifted, \n",
    "                                                                                                   reg_lambda=1, learning_rate=1e-1,\n",
    "                                                                                                   init='svd', batch_size=-1,\n",
    "                                                                                                   max_steps=10000, log_every=10, \n",
    "                                                                                                   eval_every=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the validation and training losses over (training) time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### YOUR PLOTTING CODE HERE ###\n",
    "x_axis = np.arange(0,len(tr_l_g_sweep),1)\n",
    "plt.plot(x_axis, tr_l_g_sweep)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Training loss')\n",
    "plt.title('Gradient descent training loss')\n",
    "plt.show()\n",
    "plt.plot(x_axis, val_l_g_sweep)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Validation loss')\n",
    "plt.title('Gradient descent validation loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Learn the optimal $P$ and $Q$ using the original stochastic gradient descent (mini-batches of size 1). That is, during each iteration you sample a single random training example $r_{xi}$ and update only the respective affected parameters $\\mathbf{p_x}$ and $\\mathbf{q}_i$. Set the learning rate to 0.01 and keep the other parameters as in a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, training loss: 2829198.7829536973, validation loss: 2482.7841829032586\n",
      "Iteration 500, training loss: 2732146.450935768, validation loss: 2369.8046905680567\n",
      "Iteration 1000, training loss: 2660033.8835901096, validation loss: 2293.829533713934\n",
      "Iteration 1500, training loss: 2559573.5900630914, validation loss: 2205.2641971610597\n",
      "Iteration 2000, training loss: 2517816.9983612713, validation loss: 2146.7522297541923\n",
      "Iteration 2500, training loss: 2472007.6760896076, validation loss: 2115.066402984694\n",
      "Iteration 3000, training loss: 2419053.665694773, validation loss: 2046.7265757765847\n",
      "Iteration 3500, training loss: 2344020.822977785, validation loss: 1978.5589178080893\n",
      "Iteration 4000, training loss: 2289824.5502550714, validation loss: 1920.7112590515485\n",
      "Iteration 4500, training loss: 2239819.6700077667, validation loss: 1859.17942810366\n",
      "Iteration 5000, training loss: 2195160.6129599875, validation loss: 1803.4182456181707\n",
      "Iteration 5500, training loss: 2170035.0937102204, validation loss: 1787.2014123404124\n",
      "Iteration 6000, training loss: 2139278.426107923, validation loss: 1759.2328705650843\n",
      "Iteration 6500, training loss: 2106122.8949939776, validation loss: 1729.2596080407745\n",
      "Iteration 7000, training loss: 2068214.7468194338, validation loss: 1703.8056307326992\n",
      "Iteration 7500, training loss: 2033277.6255916161, validation loss: 1673.516038026857\n",
      "Iteration 8000, training loss: 2004919.28531658, validation loss: 1644.1945916591442\n",
      "Iteration 8500, training loss: 1970416.6234985422, validation loss: 1616.0669169988134\n",
      "Iteration 9000, training loss: 1951396.1171369394, validation loss: 1596.1882897870973\n",
      "Iteration 9500, training loss: 1915831.9142394024, validation loss: 1562.318968629625\n",
      "Iteration 10000, training loss: 1895616.52449629, validation loss: 1540.031518315481\n",
      "Iteration 10500, training loss: 1848436.59615123, validation loss: 1508.338532001561\n",
      "Iteration 11000, training loss: 1821664.253160822, validation loss: 1474.8512511661763\n",
      "Iteration 11500, training loss: 1799717.2049990625, validation loss: 1453.063270464907\n",
      "Iteration 12000, training loss: 1778608.8588508172, validation loss: 1437.5767405433846\n",
      "Iteration 12500, training loss: 1755134.8470986488, validation loss: 1412.319786867763\n",
      "Iteration 13000, training loss: 1724866.2544782045, validation loss: 1383.2271749671627\n",
      "Iteration 13500, training loss: 1694371.2217055783, validation loss: 1351.9049120422549\n",
      "Iteration 14000, training loss: 1682803.9240590152, validation loss: 1343.0718969696861\n",
      "Iteration 14500, training loss: 1663686.9367795752, validation loss: 1328.4487586267765\n",
      "Iteration 15000, training loss: 1643933.0093547811, validation loss: 1308.4443017660542\n",
      "Iteration 15500, training loss: 1617860.9766584504, validation loss: 1281.9810603596031\n",
      "Iteration 16000, training loss: 1597498.7006470545, validation loss: 1261.5074227225646\n",
      "Iteration 16500, training loss: 1581648.0332620922, validation loss: 1245.6247331420607\n",
      "Iteration 17000, training loss: 1567095.4811653285, validation loss: 1226.4865740795037\n",
      "Iteration 17500, training loss: 1545594.9860922634, validation loss: 1210.2027587107677\n",
      "Iteration 18000, training loss: 1530098.8174084148, validation loss: 1195.208526614102\n",
      "Iteration 18500, training loss: 1510625.0735580225, validation loss: 1175.605759363261\n",
      "Iteration 19000, training loss: 1497523.9152941487, validation loss: 1161.6797002970766\n",
      "Iteration 19500, training loss: 1484987.713853263, validation loss: 1149.6010972297909\n",
      "Iteration 20000, training loss: 1471887.4276235097, validation loss: 1141.6763319396243\n",
      "Iteration 20500, training loss: 1455889.8764837072, validation loss: 1120.1727224290225\n",
      "Iteration 21000, training loss: 1439885.1524287085, validation loss: 1110.6514780551734\n",
      "Iteration 21500, training loss: 1428929.8820527939, validation loss: 1099.180260931196\n",
      "Iteration 22000, training loss: 1417547.4816463348, validation loss: 1091.2212725159468\n",
      "Iteration 22500, training loss: 1402901.7550057012, validation loss: 1075.3604314385411\n",
      "Iteration 23000, training loss: 1391397.2480003359, validation loss: 1063.8876633804985\n",
      "Iteration 23500, training loss: 1378570.5749620937, validation loss: 1051.2012802162935\n",
      "Iteration 24000, training loss: 1370429.4146318363, validation loss: 1045.4534713356122\n",
      "Iteration 24500, training loss: 1360762.5396077356, validation loss: 1037.6733267637053\n",
      "Iteration 25000, training loss: 1350253.264523924, validation loss: 1030.7396862828464\n",
      "Iteration 25500, training loss: 1341245.2095903144, validation loss: 1021.9587478479284\n",
      "Iteration 26000, training loss: 1329408.642549757, validation loss: 1008.9852835660304\n",
      "Iteration 26500, training loss: 1319286.7098753715, validation loss: 1000.2968461798065\n",
      "Iteration 27000, training loss: 1310393.5189436506, validation loss: 990.7871139964442\n",
      "Iteration 27500, training loss: 1302299.6114058276, validation loss: 984.8928352462825\n",
      "Iteration 28000, training loss: 1293855.3243451088, validation loss: 974.1003364292236\n",
      "Iteration 28500, training loss: 1285215.957450746, validation loss: 963.2839157360875\n",
      "Iteration 29000, training loss: 1276413.7610808464, validation loss: 954.6952301233655\n",
      "Iteration 29500, training loss: 1268631.7229813288, validation loss: 948.8103665908861\n",
      "Iteration 30000, training loss: 1261381.2597822985, validation loss: 940.8139828266536\n",
      "Iteration 30500, training loss: 1253658.5925741477, validation loss: 933.1160099714737\n",
      "Iteration 31000, training loss: 1246821.6044116747, validation loss: 928.2072847096512\n",
      "Iteration 31500, training loss: 1239269.7758868115, validation loss: 919.0692457236241\n",
      "Iteration 32000, training loss: 1230506.6906722498, validation loss: 910.0538437178989\n",
      "Iteration 32500, training loss: 1223010.748438953, validation loss: 901.3687601472536\n",
      "Iteration 33000, training loss: 1214759.617007347, validation loss: 892.7900958217422\n",
      "Iteration 33500, training loss: 1208444.6483095034, validation loss: 884.6991362397478\n",
      "Iteration 34000, training loss: 1200892.0199957371, validation loss: 877.6527920407084\n",
      "Iteration 34500, training loss: 1193764.4768245944, validation loss: 868.6097262987262\n",
      "Iteration 35000, training loss: 1186580.15230161, validation loss: 860.9659551031241\n",
      "Iteration 35500, training loss: 1179453.0934613869, validation loss: 854.6315964943749\n",
      "Iteration 36000, training loss: 1173453.030058805, validation loss: 848.4228400754515\n",
      "Iteration 36500, training loss: 1167237.9512865518, validation loss: 844.6384378830667\n",
      "Iteration 37000, training loss: 1161125.454142667, validation loss: 840.9722442397508\n",
      "Converged after 36200 iterations, on average 0.014225494005376345s per iteration\n"
     ]
    }
   ],
   "source": [
    "Q_g_st, P_g_st, val_l_g_st, tr_l_g_st, conv_g_st = latent_factor_gradient_descent(M_shifted, nonzero_indices, \n",
    "                                                                                   k=30, val_idx=val_idx,\n",
    "                                                                                   val_values=val_values_shifted, \n",
    "                                                                                   reg_lambda=1, learning_rate=1e-2,\n",
    "                                                                                   init='svd', batch_size=1,\n",
    "                                                                                   max_steps=200000, log_every=500, \n",
    "                                                                                   eval_every=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the validation and training losses over (training) time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### YOUR PLOTTING CODE HERE ###\n",
    "x_axis = np.arange(0,len(tr_l_g_st),1)\n",
    "plt.plot(x_axis, tr_l_g_st)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Training loss')\n",
    "plt.title('Gradient descent training loss')\n",
    "plt.show()\n",
    "plt.plot(x_axis, val_l_g_st)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Validation loss')\n",
    "plt.title('Stochastic Gradient Descent validation loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) (**Optional**) Learn the optimal $P$ and $Q$ similarly to b) this time using larger mini-batches of size $S$, e.g. 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, training loss: 2827276.728346734, validation loss: 2483.2176258583786\n",
      "Iteration 100, training loss: 1415977.608728465, validation loss: 1119.430111300056\n",
      "Iteration 200, training loss: 1032887.3875049842, validation loss: 749.0322647743246\n",
      "Iteration 300, training loss: 861608.3225270393, validation loss: 605.6955414101676\n",
      "Iteration 400, training loss: 763664.6106751349, validation loss: 540.1946233323221\n",
      "Iteration 500, training loss: 696769.1681086554, validation loss: 490.7142762346009\n",
      "Iteration 600, training loss: 645993.4359717123, validation loss: 451.4669581489528\n",
      "Iteration 700, training loss: 605300.7399028396, validation loss: 409.079917575578\n",
      "Iteration 800, training loss: 572108.9645208666, validation loss: 392.9897140269886\n",
      "Iteration 900, training loss: 548203.4562704808, validation loss: 376.9763950719186\n",
      "Iteration 1000, training loss: 527996.5686842324, validation loss: 366.5940126135967\n",
      "Iteration 1100, training loss: 509112.7647124338, validation loss: 355.30394774996273\n",
      "Iteration 1200, training loss: 493307.4140245153, validation loss: 344.75156140947956\n",
      "Iteration 1300, training loss: 480016.86320760974, validation loss: 335.092618275516\n",
      "Iteration 1400, training loss: 466558.9181833854, validation loss: 329.59630351225655\n",
      "Iteration 1500, training loss: 456640.97950222995, validation loss: 319.97815778883495\n",
      "Iteration 1600, training loss: 447500.6736064473, validation loss: 313.2667508315935\n",
      "Iteration 1700, training loss: 438668.24415720196, validation loss: 306.4330917924676\n",
      "Iteration 1800, training loss: 430440.49819132296, validation loss: 301.390554142794\n",
      "Iteration 1900, training loss: 424130.9545067899, validation loss: 297.5676372417424\n",
      "Iteration 2000, training loss: 417077.3805797335, validation loss: 291.6244292006724\n",
      "Iteration 2100, training loss: 411571.86275886116, validation loss: 288.84878921584806\n",
      "Iteration 2200, training loss: 405789.13182100997, validation loss: 282.15445805971\n",
      "Iteration 2300, training loss: 401633.4199791108, validation loss: 280.6088374097896\n",
      "Iteration 2400, training loss: 396937.49280604994, validation loss: 277.7886453095683\n",
      "Iteration 2500, training loss: 392928.41435986914, validation loss: 275.3800629173949\n",
      "Iteration 2600, training loss: 389513.981079684, validation loss: 272.6623338274831\n",
      "Iteration 2700, training loss: 385817.48198855587, validation loss: 269.16976684991585\n",
      "Iteration 2800, training loss: 382969.64440074674, validation loss: 267.20764843140705\n",
      "Iteration 2900, training loss: 379202.6566278134, validation loss: 264.55915078023105\n",
      "Iteration 3000, training loss: 377167.08395811915, validation loss: 263.63831803876417\n",
      "Iteration 3100, training loss: 374398.85841232963, validation loss: 262.27661281132424\n",
      "Iteration 3200, training loss: 372252.2224523601, validation loss: 261.1802173632167\n",
      "Iteration 3300, training loss: 370430.03057565127, validation loss: 260.58452448324095\n",
      "Iteration 3400, training loss: 368122.571282248, validation loss: 259.4775695181209\n",
      "Iteration 3500, training loss: 366762.7408775095, validation loss: 258.1810215355576\n",
      "Iteration 3600, training loss: 364828.7622479345, validation loss: 257.41891238419237\n",
      "Iteration 3700, training loss: 363511.5903965793, validation loss: 256.83403087522765\n",
      "Iteration 3800, training loss: 361640.53382856504, validation loss: 254.94192613160504\n",
      "Iteration 3900, training loss: 360090.1956985781, validation loss: 253.29466832801322\n",
      "Iteration 4000, training loss: 358984.53735357354, validation loss: 252.7503743908655\n",
      "Iteration 4100, training loss: 357861.4197785377, validation loss: 252.71498276220714\n",
      "Iteration 4200, training loss: 356682.8571035149, validation loss: 251.77758195837305\n",
      "Iteration 4300, training loss: 356028.32879079325, validation loss: 252.2867245455322\n",
      "Iteration 4400, training loss: 355279.64080314623, validation loss: 252.8031943652252\n",
      "Iteration 4500, training loss: 353920.962257334, validation loss: 250.4706952127543\n",
      "Iteration 4600, training loss: 353120.7256224395, validation loss: 250.52972367910016\n",
      "Iteration 4700, training loss: 352601.66106479487, validation loss: 249.53365795263667\n",
      "Iteration 4800, training loss: 351636.83653302584, validation loss: 249.34866172177337\n",
      "Iteration 4900, training loss: 350779.70870052726, validation loss: 248.77630331986387\n",
      "Iteration 5000, training loss: 350260.2046578284, validation loss: 248.7456676960222\n",
      "Iteration 5100, training loss: 349469.2525704069, validation loss: 248.2653810497357\n",
      "Iteration 5200, training loss: 348896.04555207246, validation loss: 248.31116164953013\n",
      "Iteration 5300, training loss: 348200.4313648676, validation loss: 247.57758505078624\n",
      "Iteration 5400, training loss: 347665.0114558988, validation loss: 248.05207387903363\n",
      "Converged after 4450 iterations, on average 0.016072789357798174s per iteration\n"
     ]
    }
   ],
   "source": [
    "Q_g_mb, P_g_mb, val_l_g_mb, tr_l_g_mb, conv_g_mb = latent_factor_gradient_descent(M_shifted, nonzero_indices, \n",
    "                                                                                   k=30, val_idx=val_idx,\n",
    "                                                                                   val_values=val_values_shifted, \n",
    "                                                                                   reg_lambda=1, learning_rate=1e-1,\n",
    "                                                                                   init='svd', batch_size=32,\n",
    "                                                                                   max_steps=10000, log_every=100, \n",
    "                                                                                   eval_every=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the validation and training losses over (training) time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### YOUR PLOTTING CODE HERE ###\n",
    "x_axis = np.arange(0,len(tr_l_g_mb),1)\n",
    "plt.plot(x_axis, tr_l_g_mb)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Training loss')\n",
    "plt.title('Gradient descent training loss')\n",
    "plt.show()\n",
    "plt.plot(x_axis, val_l_g_mb)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Validation loss')\n",
    "plt.title('Stochastic Gradient Descent validation loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Hyperparameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning models are often heavily dependent on the hyperparameter settings, e.g. the learning rate. Here, we will try a simple random search to find good values of the latent factor dimension $k$, the batch size, learning rate, and regularization.  \n",
    "\n",
    "### Tasks:\n",
    "\n",
    "Perform a hyperparameter search to find good values for the batch size, lambda, learning rate, and latent dimension. \n",
    "\n",
    "* For the batch size, evaluate all values in [1, 32, 512, -1] (-1 corresponds to full-sweep gradient descent).\n",
    "* For $\\lambda$, randomly sample three values in the interval [0, 1).\n",
    "* For the learning rate, evaluate all values in [1, 0.1, 0.01].\n",
    "* For the latent dimension, uniformly sample three values in the interval [5,30].\n",
    "\n",
    "Perform an exhaustive search among all combinations of these values;\n",
    "\n",
    "**Hint**: This may take a while to compute. **You don't have to wait for all the models to train** -- simply use \"dummy\" code instead of actual model training (or let it train, e.g., for only one iteration) if you don't want to wait. Note that the signature of this dummy code has to match the function 'latent_factor_gradient_descent' so that we could simply plug in the actual function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_search(M_train, val_idx, val_values):\n",
    "    \"\"\"\n",
    "    Hyperparameter search using random search.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    M_train     : sp.spmatrix, shape [N, D]\n",
    "                  Input sparse matrix where the user means have not\n",
    "                  been subtracted yet. \n",
    "                  \n",
    "    val_idx     : tuple, shape [2, n_validation]\n",
    "                  The indices used for validation, where n_validation\n",
    "                  is the size of the validation set.\n",
    "                  \n",
    "    val_values  : np.array, shape [n_validation, ]\n",
    "                  Validation set values, where n_validation is the\n",
    "                  size of the validation set.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    best_conf   : tuple, (batch_size, lambda, learning_rate, latent_dimension)\n",
    "                  The best-performing hyperparameters.\n",
    "                  \n",
    "\n",
    "    \"\"\"\n",
    "    nonzero_indices = M_train.nonzero()\n",
    "    # Remove user means.\n",
    "    M_shifted, _ = shift_user_mean(M_train)\n",
    "    batches = [1, 32, 512, -1]\n",
    "    regulaizer = np.random.random_sample(3).tolist()\n",
    "    learning_rate = [1, 0.1, 0.01]\n",
    "    ks = np.random.uniform(5,30,3).tolist()\n",
    "    best_val_loss_so_far = 1e6\n",
    "    for batch in batches:\n",
    "        for lamda in regulaizer:\n",
    "            for lr in learning_rate:\n",
    "                for k in ks:\n",
    "                    print('Training with configuration: {}, {}, {}, {}'.format(batch, lamda, lr,k))\n",
    "                    best_Q, best_P, validation_losses, train_losses, converged_after = latent_factor_gradient_descent(M_shifted, nonzero_indices, \n",
    "                                                                                                                     k=k, val_idx=val_idx, \n",
    "                                                                                                                     val_values=val_values,\n",
    "                                                                                                                     reg_lambda=lamda, learning_rate=lr,\n",
    "                                                                                                                     batch_size=batch)\n",
    "                    current_best_val_loss = min(validation_losses)\n",
    "                    print('Done. Best validation loss {}'.format(current_best_val_loss))\n",
    "                    if current_best_val_loss < best_val_loss_so_far:\n",
    "                        best_val_loss_so_far = current_best_val_loss\n",
    "                        best_conf = (batch, lamda, lr, k)\n",
    "                        print('New best configuration: {}, {}, {}, {}'.format(batch, lamda, lr,k))\n",
    "                        \n",
    "                    \n",
    "    \n",
    "    ### YOUR CODE HERE ###\n",
    "        \n",
    "    print(\"Best configuration is {}\").format(best_conf)\n",
    "    return best_conf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_configuration = parameter_search(M_train, val_idx, val_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output the best hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### YOUR CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparison of gradient descent and alternating optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the latent factor model with both alternating optimization and gradient descent, we now compare their results on the training, validation, and test set.\n",
    "\n",
    "### Tasks\n",
    "\n",
    "* Compare the root mean square errors (RMSE) for the training, validation, and test sets different settings of $k$ for both alternating optimization and gradient descent. What do you observe?\n",
    "* Compare the test RMSE for the alternating optimization model and the gradient descent model. Which performs better?\n",
    "* Plot the predicted ratings\n",
    "\n",
    "**Hint**: The output values and plots below are the ones we got when testing this sheet. Yours may be different, but if your validation or test RMSE values are larger than 1.5 or 2, it is likely that you have a bug in your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### YOUR CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### YOUR CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plots: Prediction vs. ground truth ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### YOUR PLOTTING CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### YOUR PLOTTING CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### YOUR PLOTTING CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### YOUR PLOTTING CODE HERE ###\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
